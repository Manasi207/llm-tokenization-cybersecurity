# Tokenization in Large Language Models (LLMs)

This repository explores tokenization â€” the foundational step that enables LLMs to understand and generate language.
It bridges theory, implementation, research insights, and cybersecurity applications.

Focus Areas:

- Tokenization components & pipeline
- Research-backed methods (BPE, WordPiece, Unigram)
- LLM-specific tokenizers (GPT, BERT, LLaMA)
- Cybersecurity relevance: detection, defense, and adversarial risks
