# Tokenization in BERT

BERT uses WordPiece tokenization.

## Features

- Masked language modeling compatible
- Uses special tokens like [CLS], [SEP]
- Context-aware embeddings

## Impact

- Strong understanding of sentence-level meaning
- Sensitive to token alignment

BERTâ€™s tokenizer is optimized for comprehension, not generation.
